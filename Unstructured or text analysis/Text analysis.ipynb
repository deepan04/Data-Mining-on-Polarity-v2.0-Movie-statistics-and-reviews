{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 3 : Textual analysis of movie reviews\n",
    "\n",
    "** Due Date: November 17, 2016 5:59PM**\n",
    "\n",
    "*------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.conversational-technologies.com/nldemos/nlWordle.GIF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    Deepan Sanghavi\n",
    "    Rohit pal Singh\n",
    "    Bhakti Chedda\n",
    "    Karan Somaiah Napanda\n",
    "    Dhaval Dholakia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desired outcome of the case study.**\n",
    "* In this case study we will look at movie reviews from the v2.0 polarity dataset comes from\n",
    "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
    "    * It contains written reviews of movies divided into positive and negative reviews.\n",
    "* As in Case Study 2 idea is to *analyze* the data set, make *conjectures*, support or refute those conjectures with *data*, and *tell a story* about the data!\n",
    "    \n",
    "**Required Readings:** \n",
    "* This case study will be based upon the scikit-learn Python library\n",
    "* We will build upon the turtorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "**Case study assumptions:**\n",
    "* You have access to a python installation\n",
    "\n",
    "**Required Python libraries:**\n",
    "* Numpy (www.numpy.org) (should already be installed from Case Study 2)\n",
    "* Matplotlib (matplotlib.org) (should already be installed from Case Study 2)\n",
    "* Scikit-learn (scikit-learn.org) (avaiable from Enthought Canopy)\n",
    "* You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (20 points): Complete Exercise 2: Sentiment Analysis on movie reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assuming that you have downloaded the scikit-learn source code:\n",
    "    * The data cane be downloaded using doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
    "    * A skeleton for the solution can be found in doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py\n",
    "    * A completed solution can be found in doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
    "* **It is ok to use the solution provided in the scikit-learn distribution as a starting place for your work.**\n",
    "\n",
    "### Modify the solution to Exercise 2 so that it can run in this iPython notebook\n",
    "* This will likely involved moving around data files and/or small modifications to the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Build a sentiment analysis / polarity model\n",
    "\n",
    "Sentiment analysis can be casted as a binary text classification problem,\n",
    "that is fitting a linear classifier on features extracted from the text\n",
    "of the user messages so as to guess wether the opinion of the author is\n",
    "positive or negative.\n",
    "\n",
    "In this examples we will use a movie review dataset.\n",
    "\n",
    "\"\"\"\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: Simplified BSD\n",
    "\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # NOTE: we put the following in a 'if __name__ == \"__main__\"' protected\n",
    "    # block to be able to use a multi-core grid search that also works under\n",
    "    # Windows, see: http://docs.python.org/library/multiprocessing.html#windows\n",
    "    # The multiprocessing module is used as the backend of joblib.Parallel\n",
    "    # that is used when n_jobs != 1 in GridSearchCV\n",
    "\n",
    "    # the training data folder must be passed as first argument\n",
    "    movie_reviews_data_folder = 'C:/Users/Deepan Sanghavi/501/Case Study 3/data/txt_sentoken'\n",
    "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "    print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "    \n",
    "\n",
    "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "\n",
    "    # TASK: print the cross-validated scores for the each parameters set\n",
    "    # explored by the grid search\n",
    "\n",
    "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
    "    # named y_predicted\n",
    "\n",
    "    # Print the classification report\n",
    "    ###print(metrics.classification_report(y_test, y_predicted,target_names=dataset.target_names))\n",
    "\n",
    "    # Print and plot the confusion matrix\n",
    "    ###cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    ###print(cm)\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.matshow(cm)\n",
    "    # plt.show()\n",
    "len(docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 35150)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text preprocessing, tokenizing and filtering of stopwords are included in a high level component that is able to build a \n",
    "#dictionary of features and transform documents to feature vectors:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer() #Convert a collection of text documents to a matrix of token counts\n",
    "X_train_counts = count_vect.fit_transform(docs_train) #Learn the vocabulary dictionary and return term-document matrix\n",
    "X_train_counts.shape\n",
    "#count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 35150)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#erm Frequency times Inverse Document Frequency\n",
    "#divide the number of occurrences of each word in a document by the total number of words in the document\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts) #downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus\n",
    "X_train_tf = tf_transformer.transform(X_train_counts) \n",
    "X_train_tf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 35150)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same as above but in a shorter way\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sonja</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>circuitry</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spiders</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bazooms</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>francesco</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>francesca</td>\n",
       "      <td>6.927592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>comically</td>\n",
       "      <td>6.116662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>localized</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hennings</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>canet</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scold</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>originality</td>\n",
       "      <td>4.507224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>caned</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rickman</td>\n",
       "      <td>6.234445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>workaday</td>\n",
       "      <td>6.927592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>capoeira</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rawhide</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>taj</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bringing</td>\n",
       "      <td>4.552687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>liaisons</td>\n",
       "      <td>6.927592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>grueling</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sommerset</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wooden</td>\n",
       "      <td>4.650325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wednesday</td>\n",
       "      <td>6.522127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>woods</td>\n",
       "      <td>4.758539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tak</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>crotch</td>\n",
       "      <td>6.116662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>elgar</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stereotypical</td>\n",
       "      <td>4.981682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>shows</td>\n",
       "      <td>2.780497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35120</th>\n",
       "      <td>ez</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35121</th>\n",
       "      <td>meaningless</td>\n",
       "      <td>5.748937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35122</th>\n",
       "      <td>gnawing</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35123</th>\n",
       "      <td>bombarding</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35124</th>\n",
       "      <td>goodness</td>\n",
       "      <td>5.915992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35125</th>\n",
       "      <td>kant</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35126</th>\n",
       "      <td>boop</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35127</th>\n",
       "      <td>boot</td>\n",
       "      <td>5.222844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35128</th>\n",
       "      <td>illinois</td>\n",
       "      <td>6.927592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35129</th>\n",
       "      <td>hutter</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35130</th>\n",
       "      <td>book</td>\n",
       "      <td>3.238713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35131</th>\n",
       "      <td>boom</td>\n",
       "      <td>6.234445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35132</th>\n",
       "      <td>boon</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35133</th>\n",
       "      <td>boob</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35134</th>\n",
       "      <td>honorary</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35135</th>\n",
       "      <td>figueras</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35136</th>\n",
       "      <td>lance</td>\n",
       "      <td>5.674830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35137</th>\n",
       "      <td>junk</td>\n",
       "      <td>5.423515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35138</th>\n",
       "      <td>kattan</td>\n",
       "      <td>6.367977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35139</th>\n",
       "      <td>moguls</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35140</th>\n",
       "      <td>gynecologist</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35141</th>\n",
       "      <td>june</td>\n",
       "      <td>5.828980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35142</th>\n",
       "      <td>jung</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35143</th>\n",
       "      <td>digressions</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35144</th>\n",
       "      <td>contenda</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35145</th>\n",
       "      <td>rotting</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35146</th>\n",
       "      <td>pods</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35147</th>\n",
       "      <td>emery</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35148</th>\n",
       "      <td>expands</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35149</th>\n",
       "      <td>untalented</td>\n",
       "      <td>6.522127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Words   Weights\n",
       "0              sonja  7.620740\n",
       "1          circuitry  7.215275\n",
       "2            spiders  7.215275\n",
       "3            bazooms  7.620740\n",
       "4          francesco  7.215275\n",
       "5          francesca  6.927592\n",
       "6          comically  6.116662\n",
       "7          localized  7.620740\n",
       "8           hennings  7.620740\n",
       "9              canet  7.620740\n",
       "10             scold  7.620740\n",
       "11       originality  4.507224\n",
       "12             caned  7.620740\n",
       "13           rickman  6.234445\n",
       "14          workaday  6.927592\n",
       "15          capoeira  7.620740\n",
       "16           rawhide  7.215275\n",
       "17               taj  7.620740\n",
       "18          bringing  4.552687\n",
       "19          liaisons  6.927592\n",
       "20          grueling  7.620740\n",
       "21         sommerset  7.620740\n",
       "22            wooden  4.650325\n",
       "23         wednesday  6.522127\n",
       "24             woods  4.758539\n",
       "25               tak  7.620740\n",
       "26            crotch  6.116662\n",
       "27             elgar  7.215275\n",
       "28     stereotypical  4.981682\n",
       "29             shows  2.780497\n",
       "...              ...       ...\n",
       "35120             ez  7.620740\n",
       "35121    meaningless  5.748937\n",
       "35122        gnawing  7.215275\n",
       "35123     bombarding  7.620740\n",
       "35124       goodness  5.915992\n",
       "35125           kant  7.620740\n",
       "35126           boop  7.620740\n",
       "35127           boot  5.222844\n",
       "35128       illinois  6.927592\n",
       "35129         hutter  7.620740\n",
       "35130           book  3.238713\n",
       "35131           boom  6.234445\n",
       "35132           boon  7.620740\n",
       "35133           boob  7.620740\n",
       "35134       honorary  7.215275\n",
       "35135       figueras  7.620740\n",
       "35136          lance  5.674830\n",
       "35137           junk  5.423515\n",
       "35138         kattan  6.367977\n",
       "35139         moguls  7.620740\n",
       "35140   gynecologist  7.215275\n",
       "35141           june  5.828980\n",
       "35142           jung  7.215275\n",
       "35143    digressions  7.215275\n",
       "35144       contenda  7.620740\n",
       "35145        rotting  7.215275\n",
       "35146           pods  7.215275\n",
       "35147          emery  7.620740\n",
       "35148        expands  7.215275\n",
       "35149     untalented  6.522127\n",
       "\n",
       "[35150 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#words and weight for train (same as test)\n",
    "words=count_vect.get_feature_names()\n",
    "weight=tfidf_transformer.idf_\n",
    "wordweight=pd.DataFrame((dict(zip(words,weight))).items(),columns=[\"Words\",\"Weights\"])\n",
    "wordweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "#apply bayes classifier on test data and find out predicted outcome\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "X_new_counts = count_vect.transform(docs_test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "#below two are same as above two but this is with pipeline\n",
    "#building a pipeline and train the model\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(docs_train, y_train)\n",
    "#for doc, category in zip(docs_test, predicted):\n",
    "#    print('%r => %s' % (doc, dataset.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sonja</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>circuitry</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spiders</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bazooms</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>francesco</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>francesca</td>\n",
       "      <td>6.927592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>comically</td>\n",
       "      <td>6.116662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>localized</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hennings</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>canet</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scold</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>originality</td>\n",
       "      <td>4.507224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>caned</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rickman</td>\n",
       "      <td>6.234445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>workaday</td>\n",
       "      <td>6.927592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>capoeira</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rawhide</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>taj</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bringing</td>\n",
       "      <td>4.552687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>liaisons</td>\n",
       "      <td>6.927592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>grueling</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sommerset</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wooden</td>\n",
       "      <td>4.650325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wednesday</td>\n",
       "      <td>6.522127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>woods</td>\n",
       "      <td>4.758539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tak</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>crotch</td>\n",
       "      <td>6.116662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>elgar</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stereotypical</td>\n",
       "      <td>4.981682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>shows</td>\n",
       "      <td>2.780497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35120</th>\n",
       "      <td>ez</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35121</th>\n",
       "      <td>meaningless</td>\n",
       "      <td>5.748937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35122</th>\n",
       "      <td>gnawing</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35123</th>\n",
       "      <td>bombarding</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35124</th>\n",
       "      <td>goodness</td>\n",
       "      <td>5.915992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35125</th>\n",
       "      <td>kant</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35126</th>\n",
       "      <td>boop</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35127</th>\n",
       "      <td>boot</td>\n",
       "      <td>5.222844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35128</th>\n",
       "      <td>illinois</td>\n",
       "      <td>6.927592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35129</th>\n",
       "      <td>hutter</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35130</th>\n",
       "      <td>book</td>\n",
       "      <td>3.238713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35131</th>\n",
       "      <td>boom</td>\n",
       "      <td>6.234445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35132</th>\n",
       "      <td>boon</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35133</th>\n",
       "      <td>boob</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35134</th>\n",
       "      <td>honorary</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35135</th>\n",
       "      <td>figueras</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35136</th>\n",
       "      <td>lance</td>\n",
       "      <td>5.674830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35137</th>\n",
       "      <td>junk</td>\n",
       "      <td>5.423515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35138</th>\n",
       "      <td>kattan</td>\n",
       "      <td>6.367977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35139</th>\n",
       "      <td>moguls</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35140</th>\n",
       "      <td>gynecologist</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35141</th>\n",
       "      <td>june</td>\n",
       "      <td>5.828980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35142</th>\n",
       "      <td>jung</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35143</th>\n",
       "      <td>digressions</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35144</th>\n",
       "      <td>contenda</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35145</th>\n",
       "      <td>rotting</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35146</th>\n",
       "      <td>pods</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35147</th>\n",
       "      <td>emery</td>\n",
       "      <td>7.620740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35148</th>\n",
       "      <td>expands</td>\n",
       "      <td>7.215275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35149</th>\n",
       "      <td>untalented</td>\n",
       "      <td>6.522127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Words   Weights\n",
       "0              sonja  7.620740\n",
       "1          circuitry  7.215275\n",
       "2            spiders  7.215275\n",
       "3            bazooms  7.620740\n",
       "4          francesco  7.215275\n",
       "5          francesca  6.927592\n",
       "6          comically  6.116662\n",
       "7          localized  7.620740\n",
       "8           hennings  7.620740\n",
       "9              canet  7.620740\n",
       "10             scold  7.620740\n",
       "11       originality  4.507224\n",
       "12             caned  7.620740\n",
       "13           rickman  6.234445\n",
       "14          workaday  6.927592\n",
       "15          capoeira  7.620740\n",
       "16           rawhide  7.215275\n",
       "17               taj  7.620740\n",
       "18          bringing  4.552687\n",
       "19          liaisons  6.927592\n",
       "20          grueling  7.620740\n",
       "21         sommerset  7.620740\n",
       "22            wooden  4.650325\n",
       "23         wednesday  6.522127\n",
       "24             woods  4.758539\n",
       "25               tak  7.620740\n",
       "26            crotch  6.116662\n",
       "27             elgar  7.215275\n",
       "28     stereotypical  4.981682\n",
       "29             shows  2.780497\n",
       "...              ...       ...\n",
       "35120             ez  7.620740\n",
       "35121    meaningless  5.748937\n",
       "35122        gnawing  7.215275\n",
       "35123     bombarding  7.620740\n",
       "35124       goodness  5.915992\n",
       "35125           kant  7.620740\n",
       "35126           boop  7.620740\n",
       "35127           boot  5.222844\n",
       "35128       illinois  6.927592\n",
       "35129         hutter  7.620740\n",
       "35130           book  3.238713\n",
       "35131           boom  6.234445\n",
       "35132           boon  7.620740\n",
       "35133           boob  7.620740\n",
       "35134       honorary  7.215275\n",
       "35135       figueras  7.620740\n",
       "35136          lance  5.674830\n",
       "35137           junk  5.423515\n",
       "35138         kattan  6.367977\n",
       "35139         moguls  7.620740\n",
       "35140   gynecologist  7.215275\n",
       "35141           june  5.828980\n",
       "35142           jung  7.215275\n",
       "35143    digressions  7.215275\n",
       "35144       contenda  7.620740\n",
       "35145        rotting  7.215275\n",
       "35146           pods  7.215275\n",
       "35147          emery  7.620740\n",
       "35148        expands  7.215275\n",
       "35149     untalented  6.522127\n",
       "\n",
       "[35150 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words and weight for test\n",
    "words=count_vect.get_feature_names()\n",
    "weight=tfidf_transformer.idf_\n",
    "wordweight=pd.DataFrame((dict(zip(words,weight))).items(),columns=[\"Words\",\"Weights\"])\n",
    "wordweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.80      0.89      0.84       251\n",
      "        pos       0.88      0.78      0.82       249\n",
      "\n",
      "avg / total       0.84      0.83      0.83       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#metrics\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted,target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2),(1,3)],'tfidf__use_idf': (True, False),'clf__alpha': (1e-2, 1e-3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "gs_clf = gs_clf.fit(docs_train,y_train)\n",
    "#predict\n",
    "predictedgrid=gs_clf.predict(docs_test)\n",
    "predictedgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82599999999999996"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.01\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[215,  36],\n",
       "       [ 41, 208]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predictedgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.367333</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'tfidf__use_id...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.760479</td>\n",
       "      <td>0.998999</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.765531</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.019154</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.021313</td>\n",
       "      <td>8.164975e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.096667</td>\n",
       "      <td>1.407000</td>\n",
       "      <td>0.795333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 2), u'tfidf__use_id...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.804391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.762</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.819639</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077809</td>\n",
       "      <td>0.052656</td>\n",
       "      <td>0.024378</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.083667</td>\n",
       "      <td>2.221667</td>\n",
       "      <td>0.814667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 3), u'tfidf__use_id...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.829659</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056923</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.163333</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.993330</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'tfidf__use_id...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.794411</td>\n",
       "      <td>0.988989</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.801603</td>\n",
       "      <td>0.998002</td>\n",
       "      <td>0.019137</td>\n",
       "      <td>0.017049</td>\n",
       "      <td>0.019083</td>\n",
       "      <td>3.686952e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.863667</td>\n",
       "      <td>1.351333</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 2), u'tfidf__use_id...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.818363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.843687</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066219</td>\n",
       "      <td>0.035650</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.951333</td>\n",
       "      <td>2.093667</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 3), u'tfidf__use_id...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.840319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.859719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.123990</td>\n",
       "      <td>0.041484</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.185000</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.729333</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'tfidf__use_id...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.750499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.733467</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029631</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>0.019216</td>\n",
       "      <td>4.714045e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.146333</td>\n",
       "      <td>1.442000</td>\n",
       "      <td>0.778667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 2), u'tfidf__use_id...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.794411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.787575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008994</td>\n",
       "      <td>0.029029</td>\n",
       "      <td>0.017664</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.335667</td>\n",
       "      <td>2.217667</td>\n",
       "      <td>0.799333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 3), u'tfidf__use_id...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.828343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.811623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071341</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>0.030014</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.163667</td>\n",
       "      <td>0.500667</td>\n",
       "      <td>0.764667</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'tfidf__use_id...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.774451</td>\n",
       "      <td>0.998999</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.781563</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.023099</td>\n",
       "      <td>0.013888</td>\n",
       "      <td>0.019078</td>\n",
       "      <td>9.428089e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.921000</td>\n",
       "      <td>1.356333</td>\n",
       "      <td>0.800667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 2), u'tfidf__use_id...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.812375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.762</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.827655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041239</td>\n",
       "      <td>0.033510</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.775000</td>\n",
       "      <td>1.899667</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 3), u'tfidf__use_id...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.836327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.833667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075794</td>\n",
       "      <td>0.061266</td>\n",
       "      <td>0.026892</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        1.367333         0.505000         0.748000          0.999000   \n",
       "1        5.096667         1.407000         0.795333          1.000000   \n",
       "2       11.083667         2.221667         0.814667          1.000000   \n",
       "3        1.163333         0.507000         0.784667          0.993330   \n",
       "4        4.863667         1.351333         0.813333          1.000000   \n",
       "5       10.951333         2.093667         0.826000          1.000000   \n",
       "6        1.185000         0.513000         0.729333          0.999667   \n",
       "7        5.146333         1.442000         0.778667          1.000000   \n",
       "8       11.335667         2.217667         0.799333          1.000000   \n",
       "9        1.163667         0.500667         0.764667          0.998333   \n",
       "10       4.921000         1.356333         0.800667          1.000000   \n",
       "11      10.775000         1.899667         0.816000          1.000000   \n",
       "\n",
       "   param_clf__alpha param_tfidf__use_idf param_vect__ngram_range  \\\n",
       "0              0.01                 True                  (1, 1)   \n",
       "1              0.01                 True                  (1, 2)   \n",
       "2              0.01                 True                  (1, 3)   \n",
       "3              0.01                False                  (1, 1)   \n",
       "4              0.01                False                  (1, 2)   \n",
       "5              0.01                False                  (1, 3)   \n",
       "6             0.001                 True                  (1, 1)   \n",
       "7             0.001                 True                  (1, 2)   \n",
       "8             0.001                 True                  (1, 3)   \n",
       "9             0.001                False                  (1, 1)   \n",
       "10            0.001                False                  (1, 2)   \n",
       "11            0.001                False                  (1, 3)   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "0   {u'vect__ngram_range': (1, 1), u'tfidf__use_id...               11   \n",
       "1   {u'vect__ngram_range': (1, 2), u'tfidf__use_id...                7   \n",
       "2   {u'vect__ngram_range': (1, 3), u'tfidf__use_id...                3   \n",
       "3   {u'vect__ngram_range': (1, 1), u'tfidf__use_id...                8   \n",
       "4   {u'vect__ngram_range': (1, 2), u'tfidf__use_id...                4   \n",
       "5   {u'vect__ngram_range': (1, 3), u'tfidf__use_id...                1   \n",
       "6   {u'vect__ngram_range': (1, 1), u'tfidf__use_id...               12   \n",
       "7   {u'vect__ngram_range': (1, 2), u'tfidf__use_id...                9   \n",
       "8   {u'vect__ngram_range': (1, 3), u'tfidf__use_id...                6   \n",
       "9   {u'vect__ngram_range': (1, 1), u'tfidf__use_id...               10   \n",
       "10  {u'vect__ngram_range': (1, 2), u'tfidf__use_id...                5   \n",
       "11  {u'vect__ngram_range': (1, 3), u'tfidf__use_id...                2   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0            0.760479            0.998999              0.718   \n",
       "1            0.804391            1.000000              0.762   \n",
       "2            0.834331            1.000000              0.780   \n",
       "3            0.794411            0.988989              0.758   \n",
       "4            0.818363            1.000000              0.778   \n",
       "5            0.840319            1.000000              0.778   \n",
       "6            0.750499            1.000000              0.704   \n",
       "7            0.794411            1.000000              0.754   \n",
       "8            0.828343            1.000000              0.758   \n",
       "9            0.774451            0.998999              0.738   \n",
       "10           0.812375            1.000000              0.762   \n",
       "11           0.836327            1.000000              0.778   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0                0.999           0.765531            0.999001      0.019154   \n",
       "1                1.000           0.819639            1.000000      0.077809   \n",
       "2                1.000           0.829659            1.000000      0.056923   \n",
       "3                0.993           0.801603            0.998002      0.019137   \n",
       "4                1.000           0.843687            1.000000      0.066219   \n",
       "5                1.000           0.859719            1.000000      0.123990   \n",
       "6                0.999           0.733467            1.000000      0.029631   \n",
       "7                1.000           0.787575            1.000000      0.008994   \n",
       "8                1.000           0.811623            1.000000      0.071341   \n",
       "9                0.997           0.781563            0.999001      0.023099   \n",
       "10               1.000           0.827655            1.000000      0.041239   \n",
       "11               1.000           0.833667            1.000000      0.075794   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.007071        0.021313     8.164975e-07  \n",
       "1         0.052656        0.024378     0.000000e+00  \n",
       "2         0.012499        0.024587     0.000000e+00  \n",
       "3         0.017049        0.019083     3.686952e-03  \n",
       "4         0.035650        0.027039     0.000000e+00  \n",
       "5         0.041484        0.034853     0.000000e+00  \n",
       "6         0.019613        0.019216     4.714045e-04  \n",
       "7         0.029029        0.017664     0.000000e+00  \n",
       "8         0.021140        0.030014     0.000000e+00  \n",
       "9         0.013888        0.019078     9.428089e-04  \n",
       "10        0.033510        0.028044     0.000000e+00  \n",
       "11        0.061266        0.026892     0.000000e+00  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points): Explore the scikit-learn TfidVectorizer class\n",
    "\n",
    "**Read the documentation for the TfidVectorizer class at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.** \n",
    "* Define the term frequency–inverse document frequency (TF-IDF) statistic (http://en.wikipedia.org/wiki/Tf%E2%80%93idf will likely help).\n",
    "* Run the TfidVectorizer class on the training data above (docs_train).\n",
    "* Explore the min_df and max_df parameters of TfidVectorizer.  What do they mean? How do they change the features you get?\n",
    "* Explore the ngram_range parameter of TfidVectorizer.  What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "#Defined tfidf in report\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "movie_reviews_data_folder = \"C:/Users/Deepan Sanghavi/501/Case Study 3/data/txt_sentoken\"\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Words   Weights\n",
      "0           birthday during  7.620740\n",
      "1               be electric  7.620740\n",
      "2         director jeremiah  7.620740\n",
      "3               dance clubs  7.620740\n",
      "4          minutes onscreen  7.620740\n",
      "5              widowed when  7.620740\n",
      "6            pornography on  7.620740\n",
      "7                   reel if  7.620740\n",
      "8                it serious  7.620740\n",
      "9                   spiders  6.927592\n",
      "10                  say sam  7.620740\n",
      "11              touching us  7.620740\n",
      "12       sympathetic person  7.620740\n",
      "13                  reel in  7.215275\n",
      "14          ethnic tensions  7.620740\n",
      "15               of viewing  7.620740\n",
      "16         for additionally  7.215275\n",
      "17                 hennings  7.620740\n",
      "18                    canet  7.620740\n",
      "19      definite difference  7.620740\n",
      "20          well especially  6.927592\n",
      "21             lewis sandra  7.620740\n",
      "22               changes in  6.116662\n",
      "23                tread the  7.620740\n",
      "24                    ok as  7.215275\n",
      "25                    ok at  7.620740\n",
      "26             movie covers  7.620740\n",
      "27           amy heckerling  6.704449\n",
      "28                of justin  7.620740\n",
      "29            night manages  7.620740\n",
      "...                     ...       ...\n",
      "435391          be drifting  7.620740\n",
      "435392       gangsters with  7.620740\n",
      "435393            that lame  7.620740\n",
      "435394    companionship not  7.620740\n",
      "435395           rossio the  7.620740\n",
      "435396     symbolizing life  7.620740\n",
      "435397             chest is  7.620740\n",
      "435398            world out  7.620740\n",
      "435399        from uncaring  7.620740\n",
      "435400              as soon  5.095011\n",
      "435401             doesn it  6.116662\n",
      "435402                  gas  5.480673\n",
      "435403                  gar  7.620740\n",
      "435404           is helming  7.620740\n",
      "435405     existent present  7.620740\n",
      "435406          these circa  7.620740\n",
      "435407            wimp that  7.620740\n",
      "435408          defined the  7.215275\n",
      "435409           movie camp  7.620740\n",
      "435410               25 000  7.620740\n",
      "435411     scientist within  7.620740\n",
      "435412               or job  7.620740\n",
      "435413         real russian  7.620740\n",
      "435414             admit as  7.620740\n",
      "435415         paulie calls  7.215275\n",
      "435416           movie came  7.215275\n",
      "435417   onscreen chemistry  7.620740\n",
      "435418        rounders sees  7.620740\n",
      "435419            footloose  7.215275\n",
      "435420            bird with  7.620740\n",
      "\n",
      "[435421 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(min_df=1,max_df=0.95,ngram_range=(1,2))\n",
    "\n",
    "X_train_tf = tf_vectorizer.fit_transform(docs_train)\n",
    "\n",
    "idf = tf_vectorizer.idf_\n",
    "d=dict(zip(tf_vectorizer.get_feature_names(), idf))\n",
    "df=pd.DataFrame(d.items(),columns=[\"Words\",\"Weights\"])\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.850666666667\n",
      "[[214  37]\n",
      " [ 34 215]]\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "\n",
    "pipeline = Pipeline([\n",
    "('vect', TfidfVectorizer(min_df=1, max_df=0.95)),('clf', LinearSVC(C=1000)),])\n",
    "\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "parameters = {'vect__ngram_range': [(1, 3),(2,3),(1,2)]}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "grid_search.fit(docs_train, y_train)\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "print (grid_search.best_score_)\n",
    "print(metrics.confusion_matrix(y_test, y_predicted))\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, grid_search.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.798667</td>\n",
       "      <td>1.351667</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 3)}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.826</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.843687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.035508</td>\n",
       "      <td>0.045021</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.624667</td>\n",
       "      <td>1.031000</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (2, 3)}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.673569</td>\n",
       "      <td>0.022015</td>\n",
       "      <td>0.018995</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.509667</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.850667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 2)}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.840</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.861723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.157051</td>\n",
       "      <td>0.026696</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       8.798667         1.351667         0.834000               1.0   \n",
       "1       7.624667         1.031000         0.814000               1.0   \n",
       "2       3.509667         0.883000         0.850667               1.0   \n",
       "\n",
       "  param_vect__ngram_range                          params  rank_test_score  \\\n",
       "0                  (1, 3)  {u'vect__ngram_range': (1, 3)}                2   \n",
       "1                  (2, 3)  {u'vect__ngram_range': (2, 3)}                3   \n",
       "2                  (1, 2)  {u'vect__ngram_range': (1, 2)}                1   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.832335                 1.0              0.826   \n",
       "1           0.838323                 1.0              0.792   \n",
       "2           0.850299                 1.0              0.840   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0                 1.0           0.843687                 1.0      1.035508   \n",
       "1                 1.0           0.811623                 1.0      0.673569   \n",
       "2                 1.0           0.861723                 1.0      0.157051   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.045021        0.007313              0.0  \n",
       "1        0.022015        0.018995              0.0  \n",
       "2        0.026696        0.008868              0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)\n",
    "#xc.to_csv(\"C:/Users/Deepan Sanghavi/501/Case Study 3/ngram.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "## Problem 3 (20 points): Machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based upon Problem 2 pick some parameters for TfidfVectorizer\n",
    "    * \"fit\" your TfidfVectorizer using docs_train\n",
    "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
    "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
    "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
    "* Examine two classifiers provided by scikit-learn \n",
    "    * LinearSVC\n",
    "    * KNeighborsClassifier\n",
    "    * Try a number of different parameter settings for each and judge your performance using a confusion matrix (see Problem 1 for an example).\n",
    "* Does one classifier, or one set of parameters work better?\n",
    "    * Why do you think it might be working better?\n",
    "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
    "    * Can you conjecture on why the classifier made a mistake for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "movie_reviews_data_folder = \"C:/Users/Deepan Sanghavi/501/Case Study 3/data/txt_sentoken\"\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(min_df=1,max_df=0.95)\n",
    "\n",
    "X_train_tf = tf_vectorizer.fit_transform(docs_train)\n",
    "\n",
    "idf = tf_vectorizer.idf_\n",
    "d=dict(zip(tf_vectorizer.get_feature_names(), idf))\n",
    "df=pd.DataFrame(d.items(),columns=[\"Words\",\"Weights\"])\n",
    "#print df\n",
    "voc=tf_vectorizer.vocabulary_\n",
    "vocab=pd.DataFrame(voc.items(),columns=[\"Words\",\"Count\"])\n",
    "#vocab.sort(\"Count\",ascending=False)\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.704666666667\n",
      "[[198  54]\n",
      " [ 93 155]]\n",
      "vect__ngram_range: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "#Knearest Neigbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import neighbors\n",
    "pipeline = Pipeline([\n",
    "('vect', TfidfVectorizer(min_df=1, max_df=0.95)),('clf', neighbors.KNeighborsClassifier(n_neighbors=6))])#neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "parameters = {\n",
    "        'vect__ngram_range': [(1, 3), (1, 2),(2,3)]}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "x=grid_search.fit(docs_train, y_train)\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "print (grid_search.best_score_)\n",
    "print(metrics.confusion_matrix(y_test, y_predicted))\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, grid_search.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834\n",
      "[[216  36]\n",
      " [ 41 207]]\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "\n",
    "pipeline = Pipeline([\n",
    "('vect', TfidfVectorizer(min_df=1, max_df=0.95)),('clf', LinearSVC(C=1000)),])\n",
    "\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "parameters = {'vect__ngram_range': [(1, 3),(2,3),(1,2)]}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "grid_search.fit(docs_train, y_train)\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "print (grid_search.best_score_)\n",
    "print(metrics.confusion_matrix(y_test, y_predicted))\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, grid_search.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of incorrect results\n",
    "\n",
    "import pandas as pd\n",
    "#predicted results\n",
    "testdataframegrid=pd.DataFrame(docs_test)\n",
    "testdataframegrid[\"Predicted_Results\"]=y_predicted\n",
    "testdataframegrid[\"Actual_Results\"]=y_test\n",
    "testdataframegrid=testdataframegrid.replace(to_replace=[0,1], value=[\"Negative\",\"Positive\"])\n",
    "len(testdataframegrid)\n",
    "#Incorrect results\n",
    "\n",
    "incorrectprediction=testdataframegrid[testdataframegrid.Predicted_Results<>testdataframegrid.Actual_Results]\n",
    "incorrectprediction=incorrectprediction.iloc[0:10,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "## Problem 4 (20 points): Open Ended Question:  Finding the right plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you find a two dimensional plot in which the positive and negative reviews are separated?\n",
    "    * This problem is hard since you will likely have thousands of features for review, and you will need to transform these thousands of features into just two numbers (so that you can make a 2D plot).\n",
    "* Note, I was not able to find such a plot myself!\n",
    "    * So, this problem is about **trying** but perhaps **not necessarily succeeding**!\n",
    "* I tried two things, neither of which worked very well.\n",
    "    * I first plotted the length of the review versus the number of features we compute that are in that review\n",
    "    * Second I used Principle Component Analysis on a subset of the features.\n",
    "* Can you do better than I did!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Did not display the results as it takes long time and also plotted visualization in R. Saved output in csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train Negative Comments\n",
    "import pandas as pd\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(min_df=1,max_df=0.95)\n",
    "\n",
    "X_train_tf = tf_vectorizer.fit_transform(Negativetrain.Comments)\n",
    "\n",
    "idf = tf_vectorizer.idf_\n",
    "d=dict(zip(tf_vectorizer.get_feature_names(), idf))\n",
    "ndf=pd.DataFrame(d.items(),columns=[\"Words\",\"Weights\"])\n",
    "#print df\n",
    "voc=tf_vectorizer.vocabulary_\n",
    "nvocab=pd.DataFrame(voc.items(),columns=[\"Words\",\"Count\"])\n",
    "#nvocab.sort(\"Count\",ascending=False)\n",
    "#nvocab\n",
    "#ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train Positive Comments\n",
    "import pandas as pd\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(min_df=1,max_df=0.95)\n",
    "\n",
    "X_train_tf = tf_vectorizer.fit_transform(Positivetrain.Comments)\n",
    "\n",
    "idf = tf_vectorizer.idf_\n",
    "d=dict(zip(tf_vectorizer.get_feature_names(), idf))\n",
    "pdf=pd.DataFrame(d.items(),columns=[\"Words\",\"Weights\"])\n",
    "#print df\n",
    "voc=tf_vectorizer.vocabulary_\n",
    "pvocab=pd.DataFrame(voc.items(),columns=[\"Words\",\"Count\"])\n",
    "#pvocab.sort(\"Count\",ascending=False)\n",
    "#pvocab\n",
    "#pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Common words\n",
    "common = pvocab.merge(nvocab,on=['Words'])\n",
    "filpos=pvocab[(~pvocab.Words.isin(common.Words))]\n",
    "filneg=nvocab[(~nvocab.Words.isin(common.Words))]\n",
    "print len(filpos)\n",
    "filneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logic1 dont run it takes long time\n",
    "import pandas as pd\n",
    "#Analysis\n",
    "#final=pd.DataFrame(columns=[\"Mean\",\"MinMax\",\"Predicted_Result\",\"Actual_Result\"])\n",
    "#j=0\n",
    "#for z in range(0,len(incorrectprediction)):\n",
    "    #incresult1=pd.DataFrame(columns=[\"Words\",\"Weights\"])\n",
    "final1=pd.DataFrame(columns=[\"poscount\",\"negcount\"])\n",
    "final1[\"Results\"]=testdataframegrid.Actual_Results\n",
    "final1[\"poscount\"]=0\n",
    "final1[\"negcount\"]=0\n",
    "for z in range(0,len(testdataframegrid)):\n",
    "    s=testdataframegrid.iloc[z,0]\n",
    "    for x in range(len(filpos)): \n",
    "        if(s.find(filpos.iloc[x,0]+\" \")>0):\n",
    "            final1.iloc[z,1]= final1.iloc[z,1]+filpos.iloc[x,1]\n",
    "    for x in range(len(filneg)): \n",
    "        if(s.find(filneg.iloc[x,0]+\" \")>0):\n",
    "            final1.iloc[z,0]= final1.iloc[z,0]+filneg.iloc[x,1]\n",
    "    #print(incresult1.Weights.mean(),(incresult1.Weights.min()+incresult1.Weights.max())/2,incorrectprediction.iloc[z,1])\n",
    "final1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logic 2 dont run\n",
    "import pandas as pd\n",
    "#Analysis\n",
    "#final=pd.DataFrame(columns=[\"Mean\",\"MinMax\",\"Predicted_Result\",\"Actual_Result\"])\n",
    "#j=0\n",
    "#for z in range(0,len(incorrectprediction)):\n",
    "    #incresult1=pd.DataFrame(columns=[\"Words\",\"Weights\"])\n",
    "final2=pd.DataFrame(columns=[\"poscount\",\"negcount\"])\n",
    "final2[\"Results\"]=testdataframegrid.Actual_Results\n",
    "final2[\"poscount\"]=0\n",
    "final2[\"negcount\"]=0\n",
    "for z in range(0,len(testdataframegrid) ):\n",
    "    s=testdataframegrid.iloc[z,0]\n",
    "    for x in range(len(filpos)+\" \"): \n",
    "        if(s.find(filpos.iloc[x,0])>0):\n",
    "            final2.iloc[z,0]= final2.iloc[z,0]+1\n",
    "    for x in range(len(filneg)): \n",
    "        if(s.find(filneg.iloc[x,0])>0):\n",
    "            final2.iloc[z,1]= final2.iloc[z,1]+1\n",
    "    #print(incresult1.Weights.mean(),(incresult1.Weights.min()+incresult1.Weights.max())/2,incorrectprediction.iloc[z,1])\n",
    "final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logic 3 dont run\n",
    "import pandas as pd\n",
    "#Analysis\n",
    "#final=pd.DataFrame(columns=[\"Mean\",\"MinMax\",\"Predicted_Result\",\"Actual_Result\"])\n",
    "#j=0\n",
    "#for z in range(0,len(incorrectprediction)):\n",
    "    #incresult1=pd.DataFrame(columns=[\"Words\",\"Weights\"])\n",
    "final4=pd.DataFrame(columns=[\"poscount\",\"negcount\"])\n",
    "final4[\"Results\"]=testdataframegrid.Actual_Results\n",
    "final4[\"poscount\"]=0\n",
    "final4[\"negcount\"]=0\n",
    "for z in range(0,len(testdataframegrid)):\n",
    "    s=testdataframegrid.iloc[z,0]\n",
    "    i=0\n",
    "    for x in range(len(pvocab)): \n",
    "        if(s.find(\" \" + pvocab.iloc[x,0]+ \" \")>0):\n",
    "            final4.iloc[z,0]= final4.iloc[z,0]+pvocab.iloc[x,1]\n",
    "    for x in range(len(nvocab)): \n",
    "        if(s.find(\" \"+nvocab.iloc[x,0]+\" \")>0):\n",
    "            final4.iloc[z,1]= final4.iloc[z,1]+nvocab.iloc[x,1]\n",
    "    print(z)        \n",
    "    #print(incresult1.Weights.mean(),(incresult1.Weights.min()+incresult1.Weights.max())/2,incorrectprediction.iloc[z,1])\n",
    "final4.to_csv(\"C:/Users/Deepan Sanghavi/501/Case Study 3/Final4.csv\",sep=\",\")\n",
    "final4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#New logic 4 dont run\n",
    "import pandas as pd\n",
    "#Analysis\n",
    "#final=pd.DataFrame(columns=[\"Mean\",\"MinMax\",\"Predicted_Result\",\"Actual_Result\"])\n",
    "#j=0\n",
    "#for z in range(0,len(incorrectprediction)):\n",
    "    #incresult1=pd.DataFrame(columns=[\"Words\",\"Weights\"])\n",
    "final5=pd.DataFrame(columns=[\"poscount\",\"negcount\"])\n",
    "final5[\"Results\"]=testdataframegrid.Actual_Results\n",
    "final5[\"poscount\"]=0\n",
    "final5[\"negcount\"]=0\n",
    "for z in range(0,len(testdataframegrid)):\n",
    "    s=testdataframegrid.iloc[z,0]\n",
    "    i=0\n",
    "    for x in range(len(pdf)): \n",
    "        if(s.find(\" \" + pdf.iloc[x,0]+ \" \")>0):\n",
    "            final5.iloc[z,0]= final5.iloc[z,0]+pdf.iloc[x,1]\n",
    "    for x in range(len(ndf)): \n",
    "        if(s.find(\" \"+ndf.iloc[x,0]+\" \")>0):\n",
    "            final5.iloc[z,1]= final5.iloc[z,1]+ndf.iloc[x,1] \n",
    "    print(z)#print(incresult1.Weights.mean(),(incresult1.Weights.min()+incresult1.Weights.max())/2,incorrectprediction.iloc[z,1])\n",
    "final5.to_csv(\"C:/Users/Deepan Sanghavi/501/Case Study 3/Final5.csv\",sep=\",\")\n",
    "final5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: communicate the results (20 points)\n",
    "\n",
    "(1) (5 points) What data you collected?\n",
    "\n",
    "(2) (5 points) Why this topic is interesting or important to you? (Motivations)\n",
    "\n",
    "(3) (5 points) How did you analyse the data?\n",
    "\n",
    "(4) (5 points) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides (for 10 minutes of presentation) (20 points)\n",
    "\n",
    "\n",
    "1. (5 points) Motivation about the data collection, why the topic is interesting to you. \n",
    "\n",
    "2. (10 points) Communicating Results (figure/table)\n",
    "\n",
    "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What is the relationship between this topic and Business Intelligence?\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    "    * What conjectures did you make and how did you support or disprove them using data?\n",
    "    * Did you find anything suprising in the data?\n",
    "    * What business decision do you think this data could help answer?  Why?\n",
    "\n",
    "   (please include figures or tables in the report, **but no source code**)\n",
    "\n",
    "*Please compress all the files into a single zipped file.*\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Send an email to rcpaffenroth@wpi.edu and wliu3@wpi.edu with the subject: \"[DS501] Case study 3-TEAM NUMBER ???\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
